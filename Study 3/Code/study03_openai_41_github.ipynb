{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "438f814b-15c9-4668-99f3-9021f10c614b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph, RDFS, URIRef\n",
    "from collections import defaultdict\n",
    "import textwrap\n",
    "from rdflib.namespace import RDF, OWL, split_uri\n",
    "import time\n",
    "import os\n",
    "\n",
    "def get_safe_max_new_tokens(prompt_text, model_id, context_window=8192, buffer=50):\n",
    "    \"\"\"\n",
    "    Returns a safe max_new_tokens value so the total tokens (prompt + output)\n",
    "    do not exceed the model's context window.\n",
    "\n",
    "    Parameters:\n",
    "    - prompt_text: str, the input prompt\n",
    "    - model_id: str, the HF model identifier\n",
    "    - context_window: int, max tokens the model supports (default 4096)\n",
    "    - buffer: int, optional reserve tokens (for EOS etc.)\n",
    "\n",
    "    Returns:\n",
    "    - int: safe max_new_tokens value\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    prompt_tokens = len(tokenizer.encode(prompt_text, add_special_tokens=False))\n",
    "    available_tokens = context_window - prompt_tokens - buffer\n",
    "\n",
    "    if available_tokens < 0:\n",
    "        print(f\"‚ö†Ô∏è WARNING: Prompt exceeds context window by {-available_tokens} tokens.\")\n",
    "        return 0\n",
    "    else:\n",
    "        print(f\"‚úÖ Prompt uses {prompt_tokens} tokens. {available_tokens} tokens available for generation.\")\n",
    "        return available_tokens\n",
    "\n",
    "# final_samples input\n",
    "final_samples = {'http://example.org/src#samplingtime': ['12:49:00'],\n",
    " 'http://example.org/src#patient_cpr': ['te64687489', 'c0cef4fadfd', 'dc44b505e4e', 'afedd9d7f0', 'cdse4751d0'],\n",
    " 'http://example.org/src#analysiscode': ['DNK35312', 'NPU02070'],\n",
    " 'http://example.org/src#laboratorium_idcode': ['UKN','OUI','ESB','HDI','KPL'],\n",
    " 'http://example.org/src#referenceinterval_lowerlimit': ['50.0', '137.0','27.0'],\n",
    " 'http://example.org/src#referenceinterval_upperlimit': ['30.0','7.5','105.0'],\n",
    " 'http://example.org/src#unit': ['U/L', 'mL/min', '10^6/l', 'mg/g', 'mol/l'],\n",
    " 'http://example.org/src#rekvirent_idtype': ['sorkode','sygehusafdelingsnummer','yaugethusgbdnummer','ydernummer'],\n",
    " 'http://example.org/src#samplingdate': ['2010-12-07','2017-04-16','2023-10-27'],\n",
    " 'http://example.org/src#resulttype': ['alfanumerisk', 'numerisk'],\n",
    " 'http://example.org/src#value': ['00', 'A RhD pos', '>175', 'NEG', '137'],\n",
    " 'http://example.org/src#operator': ['stoerre_end', 'mindre_end'],\n",
    " 'http://example.org/src#resultvalidation': ['for_hoej'],\n",
    " 'http://example.org/src#rekvirent_id': ['0123815','1789AFS4611','2000A005','6620378SKADE','990202']}\n",
    "\n",
    "\n",
    "# RDF graph of the source and target ontologies\n",
    "src_graph = Graph().parse(\"flat_src_onto.owl\")\n",
    "tgt_graph = Graph().parse(\"flat_tgt_onto.owl\")\n",
    "\n",
    "# Helper to compact URIs to src: or tgt: style\n",
    "def compact_uri(uri):\n",
    "    uri_str = str(uri)\n",
    "    if uri_str.startswith(\"http://example.org/src#\"):\n",
    "        return f\"src:{uri_str.split('#')[-1]}\"\n",
    "    elif uri_str.startswith(\"http://example.org/tgt#\"):\n",
    "        return f\"tgt:{uri_str.split('#')[-1]}\"\n",
    "    elif uri_str.startswith(\"http://www.w3.org/2001/XMLSchema#\"):\n",
    "        return f\"xsd:{uri_str.split('#')[-1]}\"\n",
    "    else:\n",
    "        return uri_str\n",
    "\n",
    "def generate_src_domain_description_with_classes(graph, final_samples, minimal=False, include_examples=False):\n",
    "    lines = [\"### Source Ontology Description (`src:`)\\n\"]\n",
    "\n",
    "    # --- Add class descriptions ---\n",
    "    lines.append(\"#### Classes\\n\")\n",
    "    for cls in graph.subjects(RDF.type, OWL.Class):\n",
    "        lines.append(f\"- {compact_uri(cls)}\")\n",
    "    lines.append(\"\")\n",
    "\n",
    "    # --- Add property descriptions ---\n",
    "    lines.append(\"#### Properties\\n\")\n",
    "    for uri_str, examples in final_samples.items():\n",
    "        uri = URIRef(uri_str)\n",
    "        domain = graph.value(uri, RDFS.domain)\n",
    "        range_ = graph.value(uri, RDFS.range)\n",
    "\n",
    "        lines.append(f\"- {compact_uri(uri)}\")\n",
    "        if domain:\n",
    "            lines.append(f\"  - Domain: {compact_uri(domain)}\")\n",
    "        if range_:\n",
    "            lines.append(f\"  - Range: {compact_uri(range_)}\")\n",
    "\n",
    "        if not minimal:\n",
    "            label = graph.value(uri, RDFS.label)\n",
    "            comment = graph.value(uri, RDFS.comment)\n",
    "            if label:\n",
    "                lines.append(f\"  - Label: \\\"{str(label)}\\\"\")\n",
    "            if comment:\n",
    "                lines.append(f\"  - Comment: \\\"{str(comment)}\\\"\")\n",
    "            if include_examples and examples:\n",
    "                example_line = textwrap.fill(', '.join(examples), width=80,\n",
    "                                             initial_indent='  - Example values: ',\n",
    "                                             subsequent_indent=' ' * 20)\n",
    "                lines.append(example_line)\n",
    "        lines.append(\"\")\n",
    "\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "def generate_tgt_domain_description_with_classes(graph, minimal=False):\n",
    "    lines = [\"### Target Ontology Description (`tgt:`)\\n\"]\n",
    "\n",
    "    # --- Add class descriptions ---\n",
    "    lines.append(\"#### Classes\\n\")\n",
    "    for cls in graph.subjects(RDF.type, OWL.Class):\n",
    "        lines.append(f\"- {compact_uri(cls)}\")\n",
    "    lines.append(\"\")\n",
    "\n",
    "    # --- Add property descriptions ---\n",
    "    lines.append(\"#### Properties\\n\")\n",
    "    properties = set(graph.subjects(RDFS.domain, None)) | set(graph.subjects(RDFS.range, None))\n",
    "\n",
    "    for uri in sorted(properties):\n",
    "        domain = graph.value(uri, RDFS.domain)\n",
    "        range_ = graph.value(uri, RDFS.range)\n",
    "\n",
    "        lines.append(f\"- {compact_uri(uri)}\")\n",
    "        if domain:\n",
    "            lines.append(f\"  - Domain: {compact_uri(domain)}\")\n",
    "        if range_:\n",
    "            lines.append(f\"  - Range: {compact_uri(range_)}\")\n",
    "\n",
    "        if not minimal:\n",
    "            label = graph.value(uri, RDFS.label)\n",
    "            comment = graph.value(uri, RDFS.comment)\n",
    "            if label:\n",
    "                lines.append(f\"  - Label: \\\"{str(label)}\\\"\")\n",
    "            if comment:\n",
    "                lines.append(f\"  - Comment: \\\"{str(comment)}\\\"\")\n",
    "        lines.append(\"\")\n",
    "\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "# Generate descriptions including examples for the source\n",
    "src_domain_str = generate_src_domain_description_with_classes(src_graph, final_samples, minimal=False, include_examples=True)\n",
    "tgt_domain_str = generate_tgt_domain_description_with_classes(tgt_graph, minimal=False)\n",
    "\n",
    "minimal_src_domain_str = generate_src_domain_description_with_classes(src_graph, final_samples, minimal=True, include_examples=False)\n",
    "minimal_tgt_domain_str = generate_tgt_domain_description_with_classes(tgt_graph, minimal=True)\n",
    "\n",
    "src_domain_str_wo_examples = generate_src_domain_description_with_classes(src_graph, final_samples, minimal=False, include_examples=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a399764-33d3-4698-a0a9-da0037a1e357",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_key='' #insert API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "957d59e4-f1bf-4793-ae3a-8124641b6c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "client = OpenAI(api_key=openai_key)\n",
    "gpt_model = \"gpt-4.1\"\n",
    "\n",
    "def call_chatgpt(prompt, system_prompt):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt}]\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model = gpt_model,\n",
    "            max_completion_tokens=8192,\n",
    "            messages = messages,\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return str(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc378aa4-f173-4f16-b357-2664bf08fe7d",
   "metadata": {},
   "source": [
    "| Model                   | Context Window       |\r\n",
    "| ----------------------- | -------------------- |\r\n",
    "| `gpt-3.5-turbo`         | 16,385 tokens        |\r\n",
    "| `gpt-4` (standard)      | 8,192 tokens         |\r\n",
    "| `gpt-4-32k`             | 32,768 tokens        |\r\n",
    "| `gpt-4o` (latest mo| 128,000 tokens       | ‚úÖ |\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc5bf7a-ab4e-421e-a3c6-de442d45ae41",
   "metadata": {},
   "source": [
    "### 1. Matching Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29871ad2-8071-4b0a-9096-481038dfe81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_system_prompt = \"You are an expert in ontology matching.\"\n",
    "match_prompt = f\"\"\"\n",
    "## Task: Ontology Matching\n",
    "\n",
    "Given a **Source Ontology** (`src:`) and a **Target Ontology** (`tgt:`), identify semantic matches: corresponding classes and properties between the ontologies. Your task is to find and list all **semantic matches**‚Äîpairs of concepts that refer to the same or closely related ideas.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† What to Match\n",
    "- Match classes or properties with the **same or closely related meaning**\n",
    "- Use similarities in **labels, comments, domains, ranges, and names**\n",
    "- Include only clear, meaningful correspondences\n",
    "\n",
    "- **Matchings may include**:\n",
    "  - 1:1 ‚Äî One source concept ‚âà One target concept  \n",
    "  - 1:N ‚Äî One source concept ‚âà Multiple target concepts  \n",
    "  - N:1 ‚Äî Multiple source concepts ‚âà One target concept  \n",
    "  - M:N ‚Äî Multiple source concepts ‚âà Multiple target concepts  \n",
    "\n",
    "- When more than one match exists for a concept, include **each match on its own line**\n",
    "- If no suitable target exists for a source concept, **omit it from the output**\n",
    "\n",
    "---\n",
    "\n",
    "### üßÆ Matching Procedure\n",
    "\n",
    "Iterate systematically:\n",
    "\n",
    "- For each `src:` concept in the Source Ontology:\n",
    "  - Compare it to each `tgt:` concept in the Target Ontology.\n",
    "  - If a semantic match is found, output a line in this format:\n",
    "    `src:ConceptA ‚âà tgt:ConceptB`\n",
    "  - If multiple matches exist for a concept, output one line per match.\n",
    "  - If no match exists, skip the concept.\n",
    "\n",
    "---\n",
    "\n",
    "### üîç What Qualifies as a Semantic Match\n",
    "A **semantic match** exists when two concepts refer to the same or equivalent meaning ‚Äî even if they differ in structure or representation. Valid matches include:\n",
    "- One concept being a **component** of another (e.g., `src:firstname` vs. `tgt:fullname`)\n",
    "- Cases where **converting**, **parsing**, or **value transformation** would be required\n",
    "- Multiple target concepts jointly representing a source concept\n",
    "\n",
    "You are not required to define the transformation ‚Äî only to identify that a match exists.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ Examples of Valid Matches\n",
    "- src:birthDate ‚âà tgt:date_of_birth  \n",
    "- src:birthDate ‚âà tgt:personAge # Requires calculating age from birth date\n",
    "- src:height_m ‚âà tgt:personHeight_feet # Requires unit conversion (meters ‚Üí feet)\n",
    "- src:employmentStatus ‚âà tgt:employment_status # Requires a lookup: src: uses integers (e.g. 1, 2), tgt: uses strings (e.g. \"employed\", \"unemployed\")\n",
    "\n",
    "---\n",
    "\n",
    "### üóÇÔ∏è Ontologies\n",
    "\n",
    "**Source Ontology** (`src:`):  \n",
    "{src_domain_str}\n",
    "\n",
    "**Target Ontology** (`tgt:`):  \n",
    "{tgt_domain_str}\n",
    "\n",
    "---\n",
    "\n",
    "### üì§ Output Format\n",
    "\n",
    "Each line must contain:\n",
    "- A `src:` element\n",
    "- The ‚âà symbol\n",
    "- A `tgt:` element (or ‚àÖ if unmatched)\n",
    " \n",
    "‚ùó Output only a list of matched pairs using this exact format:\n",
    "- src:PropertyA ‚âà tgt:PropertyB  \n",
    "- src:ClassX ‚âà tgt:ClassY  \n",
    "\n",
    "‚úÖ End your response **after you have iterated over all concepts in the Source Ontology**.\n",
    "\n",
    "- Only output the matches.\n",
    "- Output a Python list with each line as an element.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da76bf29-a4f4-4039-bd5b-db6df882c773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Time: 5.95 seconds\n",
      "Response:\n",
      " [\n",
      "\"src:Measurement ‚âà tgt:Measurement\",\n",
      "\"src:samplingtime ‚âà tgt:measurement_time\",\n",
      "\"src:samplingtime ‚âà tgt:measurement_datetime\",\n",
      "\"src:patient_cpr ‚âà tgt:person_id\",\n",
      "\"src:analysiscode ‚âà tgt:measurement_source_concept_id\",\n",
      "\"src:analysiscode ‚âà tgt:measurement_source_value\",\n",
      "\"src:laboratorium_idcode ‚âà tgt:provider_id\",\n",
      "\"src:referenceinterval_lowerlimit ‚âà tgt:range_low\",\n",
      "\"src:referenceinterval_upperlimit ‚âà tgt:range_high\",\n",
      "\"src:unit ‚âà tgt:unit_source_value\",\n",
      "\"src:samplingdate ‚âà tgt:measurement_date\",\n",
      "\"src:samplingdate ‚âà tgt:measurement_datetime\",\n",
      "\"src:value ‚âà tgt:measurement_source_value\",\n",
      "\"src:value ‚âà tgt:value_as_number\",\n",
      "\"src:value ‚âà tgt:value_as_concept_id\",\n",
      "\"src:operator ‚âà tgt:operator_concept_id\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# call chatgpt\n",
    "start_time = time.time()\n",
    "gpt4_response_match = call_chatgpt(match_prompt, match_system_prompt)\n",
    "response_time_match = time.time() - start_time\n",
    "\n",
    "print(f\"Response Time: {response_time_match:.2f} seconds\")\n",
    "print(\"Response:\\n\", gpt4_response_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec1db6f-e7c7-49db-aec6-fd9e2546e7a8",
   "metadata": {},
   "source": [
    "### 2. Mapping Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "196762c0-a1a9-4c5f-a941-ee11c74fdc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_system_prompt=\"You are a knowledge representation expert.\"\n",
    "\n",
    "mapping_prompt = f\"\"\"\n",
    "## Task: Ontology Alignment via First-Order Logic (FOL)\n",
    "\n",
    "Given a source ontology and a target ontology, produce a complete set of logical alignments between them. These alignments describe how to semantically transform data from the source to the target. Express all mappings using first-order logic (FOL).\n",
    "\n",
    "---\n",
    "\n",
    "### üóÇÔ∏è Ontology Context\n",
    "\n",
    "The source ontology includes example values for its datatype properties. The target ontology includes structural definitions only.\n",
    "\n",
    "**Source Ontology** (`src:`):  \n",
    "{src_domain_str}\n",
    "\n",
    "**Target Ontology** (`tgt:`):  \n",
    "{tgt_domain_str}\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Requirements:\n",
    "\n",
    "#### üîó Matched Elements\n",
    "\n",
    "Only generate mappings for the following matched properties and classes:\n",
    "{gpt4_response_match}\n",
    "\n",
    "---\n",
    "\n",
    "#### üîÑ Mapping Relationships\n",
    "\n",
    "- Mappings may include:\n",
    "  - 1:1 mappings (direct correspondence)\n",
    "  - 1:2 or 2:1 mappings (split or combine)\n",
    "  - n:m mappings (many-to-many relationships)\n",
    "- Mappings may involve intermediate transformation steps such as:\n",
    "  - Concatenating multiple values\n",
    "  - Decomposing or parsing a value into structured parts\n",
    "  - Mapping string or code values to concept identifiers via lookup tables\n",
    "- In addition to properties, identify and align source and target classes.\n",
    "- When instances of a source class (e.g., `src:ClassA`) should be transformed into instances of a corresponding target class (e.g., `tgt:ClassB`), include this as a class-level mapping:\n",
    "\n",
    "  ‚àÄx (src:ClassA(x) ‚Üí tgt:ClassB(x))\n",
    "\n",
    "---\n",
    "\n",
    "#### üîß Parsing Literal Values\n",
    "\n",
    "- When a literal value must be parsed into multiple structured components (e.g., extracting an operator and a number from a threshold string), assume a helper function exists to do so.\n",
    "- Select an appropriate name for the parsing function using the format:\n",
    "\n",
    "  `parse_{{value_type}}` or `parse_{{semantic_task}}`\n",
    "\n",
    "- Examples (naming conventions only):\n",
    "  - `parse_threshold`\n",
    "  - `parse_date_range`\n",
    "  - `parse_value_with_unit`\n",
    "\n",
    "- These parsing functions are to be treated as opaque and deterministic. Use them directly in the logic expressions.\n",
    "\n",
    "---\n",
    "\n",
    "#### üìö Lookup Functions for Concept Resolution\n",
    "\n",
    "- When values such as codes, units, or symbolic strings need to be mapped to concept identifiers in the target ontology, assume the existence of a lookup function.\n",
    "- Name the function according to the target resource it returns:\n",
    "\n",
    "  `lookup_{{target_resource_name}}(input_value)`\n",
    "\n",
    "- Examples (naming conventions only):\n",
    "  - `lookup_concept_id`\n",
    "  - `lookup_unit_concept`\n",
    "  - `lookup_operator_concept`\n",
    "\n",
    "- These functions return target concept identifiers and can be used directly in logical rules.\n",
    "\n",
    "---\n",
    "\n",
    "### üìò Illustrative Examples (Structure Only)\n",
    "\n",
    "The examples below demonstrate the structure of valid alignment rules and usage of helper functions. They are not tied to any specific ontology or domain.\n",
    "\n",
    "‚àÄx,v (src:hasValue(x,v) ‚àß parse_threshold(v,op,n) ‚Üí tgt:operatorConceptId(x, lookup_operator_concept(op)) ‚àß tgt:valueAsNumber(x,n))\n",
    "\n",
    "‚àÄx,c (src:hasCode(x,c) ‚Üí tgt:conceptId(x, lookup_concept_id(c)))\n",
    "\n",
    "---\n",
    "\n",
    "### üì§ Output Format\n",
    "\n",
    "- Produce a list of distinct, universally quantified first-order logic (FOL) rules that define how source predicates map to target predicates.\n",
    "- Follow this structure:\n",
    "\n",
    "  ‚àÄx,y,... (src:Triple1 ‚àß src:Triple2 ‚Üí tgt:Triple3 ‚àß tgt:Triple4 ...)\n",
    "\n",
    "- Use predicates of the form `src:propertyName` and `tgt:propertyName`.\n",
    "- Each rule must be:\n",
    "  - Syntactically valid\n",
    "  - Semantically meaningful\n",
    "  - Logically sound\n",
    "\n",
    "- Only output the FOL rules.\n",
    "- Begin your response immediately with the first FOL rule.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "027081f2-0152-433c-aebb-a095754ec8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Time: 8.16 seconds\n",
      "Response:\n",
      " ‚àÄx (src:Measurement(x) ‚Üí tgt:Measurement(x))\n",
      "\n",
      "‚àÄx,t (src:samplingtime(x,t) ‚Üí tgt:measurement_time(x,t))\n",
      "\n",
      "‚àÄx,t (src:samplingtime(x,t) ‚àß src:samplingdate(x,d) ‚àß combine_date_time(d,t,dt) ‚Üí tgt:measurement_datetime(x,dt))\n",
      "\n",
      "‚àÄx,d (src:samplingdate(x,d) ‚Üí tgt:measurement_date(x,d))\n",
      "\n",
      "‚àÄx,d (src:samplingdate(x,d) ‚àß src:samplingtime(x,t) ‚àß combine_date_time(d,t,dt) ‚Üí tgt:measurement_datetime(x,dt))\n",
      "\n",
      "‚àÄx,p (src:patient_cpr(x,p) ‚Üí tgt:person_id(x,lookup_person_id(p)))\n",
      "\n",
      "‚àÄx,a (src:analysiscode(x,a) ‚Üí tgt:measurement_source_concept_id(x,lookup_concept_id(a)))\n",
      "\n",
      "‚àÄx,a (src:analysiscode(x,a) ‚Üí tgt:measurement_source_value(x,a))\n",
      "\n",
      "‚àÄx,l (src:laboratorium_idcode(x,l) ‚Üí tgt:provider_id(x,lookup_provider_id(l)))\n",
      "\n",
      "‚àÄx,low (src:referenceinterval_lowerlimit(x,low) ‚Üí tgt:range_low(x,to_decimal(low)))\n",
      "\n",
      "‚àÄx,high (src:referenceinterval_upperlimit(x,high) ‚Üí tgt:range_high(x,to_decimal(high)))\n",
      "\n",
      "‚àÄx,u (src:unit(x,u) ‚Üí tgt:unit_source_value(x,u))\n",
      "\n",
      "‚àÄx,u (src:unit(x,u) ‚Üí tgt:unit_concept_id(x,lookup_unit_concept(u)))\n",
      "\n",
      "‚àÄx,v (src:value(x,v) ‚Üí tgt:measurement_source_value(x,v))\n",
      "\n",
      "‚àÄx,v (src:value(x,v) ‚àß is_numeric(v,n) ‚Üí tgt:value_as_number(x,to_decimal(n)))\n",
      "\n",
      "‚àÄx,v (src:value(x,v) ‚àß ¬¨is_numeric(v,_) ‚Üí tgt:value_as_concept_id(x,lookup_concept_id(v)))\n",
      "\n",
      "‚àÄx,o (src:operator(x,o) ‚Üí tgt:operator_concept_id(x,lookup_operator_concept(o)))\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "gpt4_response_map = call_chatgpt(mapping_prompt, mapping_system_prompt)\n",
    "response_time_map = time.time() - start_time\n",
    "\n",
    "print(f\"Response Time: {response_time_map:.2f} seconds\")\n",
    "print(\"Response:\\n\", gpt4_response_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4320f33-fddb-4ed1-9615-aec00bea34d5",
   "metadata": {},
   "source": [
    "### FGF Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ab0eb49-91a7-4c58-93ec-2c8b86ff4100",
   "metadata": {},
   "outputs": [],
   "source": [
    "fgf_system_prompt=\"You are an expert Python developer specializing in RDF transformations.\"\n",
    "\n",
    "fgf_prompt = f\"\"\"\n",
    "Your task is to generate **Fact Generating Functions (FGFs)** ‚Äî Python functions that use RDFLib to transform RDF triples from a source graph (`src_graph`) into a target graph (`tgt_graph`) based on a formal ontology alignment.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ Definition of FGFs\n",
    "\n",
    "Each FGF:\n",
    "- Handles a specific **source class** (e.g., `src:Person`).\n",
    "- Iterates over all instances of that class in `src_graph`.\n",
    "- Creates a corresponding instance in `tgt_graph` using the mapped target class.\n",
    "- Transfers property values using explicitly defined mappings.\n",
    "- Skips any information that is not mapped.\n",
    "- Uses the `SRC` and `TGT` namespaces to build URIs.\n",
    "- Ensures that domain and range constraints of the **target ontology** are respected.\n",
    "- Follows RDFLib syntax and best practices.\n",
    "\n",
    "---\n",
    "\n",
    "### üìë Ontology Mappings\n",
    "\n",
    "The `mappings_block` below consists of first-order logic (FOL) alignment rules describing how source classes and properties map to target ontology structures.\n",
    "\n",
    "```text\n",
    "{gpt4_response_map}\n",
    "```\n",
    "\n",
    "#### Ontology Domain and Range (for validation)\n",
    "\n",
    "**Source Ontology** (`src:`):\n",
    "{minimal_src_domain_str}\n",
    "\n",
    "## Target ontology (`tgt:`):\n",
    "{minimal_tgt_domain_str}\n",
    "\n",
    "### ‚úÖ Instructions\n",
    "\n",
    "For each source class that appears as the subject of a class-level alignment:\n",
    "1. Write a single Python function that:\n",
    "   - Finds all instances of the source class in `src_graph`.\n",
    "   - Constructs a new instance of the mapped target class in `tgt_graph`.\n",
    "   - Transfers mapped properties using the logic in mappings_block.\n",
    "2. Use RDFLib's `URIRef`, `Literal`, and `RDF.type` as appropriate.\n",
    "3. Construct URIs using `SRC` and `TGT` namespaces.\n",
    "4. Do not include unmapped properties or relationships.\n",
    "5. Ensure all triples conform to the target ontology's domain and range constraints.\n",
    "6. You may call helper functions like parse_threshold() or lookup_concept_id() if referenced in the mappings.\n",
    "7. Each function must be standalone, valid Python, and directly executable in a transformation pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Output Constraint\n",
    "\n",
    "Only output the Python function(s) corresponding to the mapped classes.   \n",
    "Output code only.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4edcda76-9803-4177-9254-a9925592edc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Time: 23.83 seconds\n",
      "Response:\n",
      " ```python\n",
      "from rdflib import URIRef, Literal, RDF, XSD\n",
      "from rdflib.namespace import Namespace\n",
      "from datetime import datetime, date, time\n",
      "\n",
      "SRC = Namespace(\"http://example.org/src#\")\n",
      "TGT = Namespace(\"http://example.org/tgt#\")\n",
      "\n",
      "def combine_date_time(date_val, time_val):\n",
      "    \"\"\"Combine xsd:date and xsd:time Literal into xsd:dateTime string.\"\"\"\n",
      "    if not (isinstance(date_val, Literal) and isinstance(time_val, Literal)):\n",
      "        return None\n",
      "    try:\n",
      "        dt = datetime.combine(date.fromisoformat(str(date_val)), time.fromisoformat(str(time_val)))\n",
      "        return Literal(dt.isoformat(), datatype=XSD.dateTime)\n",
      "    except Exception:\n",
      "        return None\n",
      "\n",
      "def to_decimal(val):\n",
      "    \"\"\"Converts a value which can be string, int etc. to a decimal-formatted Literal.\"\"\"\n",
      "    try:\n",
      "        return Literal(float(val), datatype=XSD.decimal)\n",
      "    except Exception:\n",
      "        return None\n",
      "\n",
      "def is_numeric(val):\n",
      "    \"\"\"Checks if given value (Literal) is numeric; if so, returns number as string, else None.\"\"\"\n",
      "    try:\n",
      "        n = float(str(val))\n",
      "        return True, str(n)\n",
      "    except Exception:\n",
      "        return False, None\n",
      "\n",
      "def lookup_person_id(cpr_val):\n",
      "    \"\"\"Mockup: Converts CPR string to an integer id for person_id.\"\"\"\n",
      "    # Insert actual lookup logic\n",
      "    return Literal(abs(hash(str(cpr_val))) % 10**9, datatype=XSD.integer)\n",
      "\n",
      "def lookup_concept_id(code_val):\n",
      "    \"\"\"Mockup: Convert code string to a concept id integer.\"\"\"\n",
      "    # Insert actual lookup logic\n",
      "    return Literal(abs(hash(str(code_val))) % 10**6, datatype=XSD.integer)\n",
      "\n",
      "def lookup_provider_id(lab_id_val):\n",
      "    \"\"\"Mockup: Convert laboratorium id to a provider id integer.\"\"\"\n",
      "    # Insert actual lookup logic\n",
      "    return Literal(abs(hash(str(lab_id_val))) % 10**7, datatype=XSD.integer)\n",
      "\n",
      "def lookup_unit_concept(unit_val):\n",
      "    \"\"\"Mockup: Convert unit string to a unit concept id integer.\"\"\"\n",
      "    # Insert actual lookup logic\n",
      "    return Literal(abs(hash(str(unit_val))) % 10**4, datatype=XSD.integer)\n",
      "\n",
      "def lookup_operator_concept(op_val):\n",
      "    \"\"\"Mockup: Convert operator string to an operator concept id integer.\"\"\"\n",
      "    # Insert actual lookup logic\n",
      "    return Literal(abs(hash(str(op_val))) % 10**3, datatype=XSD.integer)\n",
      "\n",
      "def transform_measurement(src_graph, tgt_graph):\n",
      "    \"\"\"\n",
      "    FGF for src:Measurement aligned to tgt:Measurement.\n",
      "    Transfers mapped properties with required transformation.\n",
      "    \"\"\"\n",
      "    for meas in src_graph.subjects(RDF.type, SRC.Measurement):\n",
      "        tgt_graph.add((meas, RDF.type, TGT.Measurement))\n",
      "        \n",
      "        # src:samplingtime(x,t) ‚Üí tgt:measurement_time(x,t)\n",
      "        for _, _, t in src_graph.triples((meas, SRC.samplingtime, None)):\n",
      "            tgt_graph.add((meas, TGT.measurement_time, Literal(t, datatype=XSD.time)))\n",
      "\n",
      "        # src:samplingdate(x,d) ‚Üí tgt:measurement_date(x,d)\n",
      "        for _, _, d in src_graph.triples((meas, SRC.samplingdate, None)):\n",
      "            tgt_graph.add((meas, TGT.measurement_date, Literal(d, datatype=XSD.date)))\n",
      "        \n",
      "        # src:samplingdate + src:samplingtime ‚Üí tgt:measurement_datetime(x,dt)\n",
      "        d = next((o for _, _, o in src_graph.triples((meas, SRC.samplingdate, None))), None)\n",
      "        t = next((o for _, _, o in src_graph.triples((meas, SRC.samplingtime, None))), None)\n",
      "        if d is not None and t is not None:\n",
      "            dt = combine_date_time(d, t)\n",
      "            if dt is not None:\n",
      "                tgt_graph.add((meas, TGT.measurement_datetime, dt))\n",
      "\n",
      "        # src:patient_cpr(x,p) ‚Üí tgt:person_id(x,lookup_person_id(p))\n",
      "        for _, _, p in src_graph.triples((meas, SRC.patient_cpr, None)):\n",
      "            tgt_graph.add((meas, TGT.person_id, lookup_person_id(p)))\n",
      "        \n",
      "        # src:analysiscode(x,a) ‚Üí tgt:measurement_source_concept_id(x,lookup_concept_id(a)), tgt:measurement_source_value(x,a)\n",
      "        for _, _, a in src_graph.triples((meas, SRC.analysiscode, None)):\n",
      "            tgt_graph.add((meas, TGT.measurement_source_concept_id, lookup_concept_id(a)))\n",
      "            tgt_graph.add((meas, TGT.measurement_source_value, Literal(a, datatype=XSD.string)))\n",
      "\n",
      "        # src:laboratorium_idcode(x,l) ‚Üí tgt:provider_id(x,lookup_provider_id(l))\n",
      "        for _, _, l in src_graph.triples((meas, SRC.laboratorium_idcode, None)):\n",
      "            tgt_graph.add((meas, TGT.provider_id, lookup_provider_id(l)))\n",
      "        \n",
      "        # src:referenceinterval_lowerlimit(x,low) ‚Üí tgt:range_low(x,to_decimal(low))\n",
      "        for _, _, low in src_graph.triples((meas, SRC.referenceinterval_lowerlimit, None)):\n",
      "            d = to_decimal(low)\n",
      "            if d is not None:\n",
      "                tgt_graph.add((meas, TGT.range_low, d))\n",
      "        \n",
      "        # src:referenceinterval_upperlimit(x,high) ‚Üí tgt:range_high(x,to_decimal(high))\n",
      "        for _, _, high in src_graph.triples((meas, SRC.referenceinterval_upperlimit, None)):\n",
      "            d = to_decimal(high)\n",
      "            if d is not None:\n",
      "                tgt_graph.add((meas, TGT.range_high, d))\n",
      "        \n",
      "        # src:unit(x,u) ‚Üí tgt:unit_source_value(x,u), tgt:unit_concept_id(x,lookup_unit_concept(u))\n",
      "        for _, _, u in src_graph.triples((meas, SRC.unit, None)):\n",
      "            tgt_graph.add((meas, TGT.unit_source_value, Literal(u, datatype=XSD.string)))\n",
      "            tgt_graph.add((meas, TGT.unit_concept_id, lookup_unit_concept(u)))\n",
      "\n",
      "        # src:value(x,v) ‚Üí tgt:measurement_source_value(x,v)\n",
      "        for _, _, v in src_graph.triples((meas, SRC.value, None)):\n",
      "            tgt_graph.add((meas, TGT.measurement_source_value, Literal(v, datatype=XSD.string)))\n",
      "\n",
      "            # src:value(x,v) ‚àß is_numeric(v,n) ‚Üí tgt:value_as_number(x,to_decimal(n))\n",
      "            isnum, n = is_numeric(v)\n",
      "            if isnum:\n",
      "                num_lit = to_decimal(n)\n",
      "                if num_lit is not None:\n",
      "                    tgt_graph.add((meas, TGT.value_as_number, num_lit))\n",
      "            else:\n",
      "                # src:value(x,v) ‚àß ¬¨is_numeric(v,_) ‚Üí tgt:value_as_concept_id(x,lookup_concept_id(v))\n",
      "                tgt_graph.add((meas, TGT.value_as_concept_id, lookup_concept_id(v)))\n",
      "\n",
      "        # src:operator(x,o) ‚Üí tgt:operator_concept_id(x,lookup_operator_concept(o))\n",
      "        for _, _, o in src_graph.triples((meas, SRC.operator, None)):\n",
      "            tgt_graph.add((meas, TGT.operator_concept_id, lookup_operator_concept(o)))\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "gpt4_response_fgf = call_chatgpt(fgf_prompt, fgf_system_prompt)\n",
    "response_time_fgf = time.time() - start_time\n",
    "\n",
    "print(f\"Response Time: {response_time_fgf:.2f} seconds\")\n",
    "print(\"Response:\\n\", gpt4_response_fgf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89463d5-e24c-4a8b-a3ee-bd144643f3de",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30241ca6-4d54-469c-b70b-e06411587726",
   "metadata": {},
   "source": [
    "### Matchings evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "453b4d29-18fa-4a7b-a6a4-0d9004658768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['src:Measurement ‚âà tgt:Measurement', 'src:samplingtime ‚âà tgt:measurement_time', 'src:samplingtime ‚âà tgt:measurement_datetime', 'src:patient_cpr ‚âà tgt:person_id', 'src:analysiscode ‚âà tgt:measurement_source_concept_id', 'src:analysiscode ‚âà tgt:measurement_source_value', 'src:laboratorium_idcode ‚âà tgt:provider_id', 'src:referenceinterval_lowerlimit ‚âà tgt:range_low', 'src:referenceinterval_upperlimit ‚âà tgt:range_high', 'src:unit ‚âà tgt:unit_source_value', 'src:samplingdate ‚âà tgt:measurement_date', 'src:samplingdate ‚âà tgt:measurement_datetime', 'src:value ‚âà tgt:measurement_source_value', 'src:value ‚âà tgt:value_as_number', 'src:value ‚âà tgt:value_as_concept_id', 'src:operator ‚âà tgt:operator_concept_id']\n",
      "Precision: 0.88\n",
      "Recall: 0.70\n",
      "F1 Score: 0.78\n"
     ]
    }
   ],
   "source": [
    "ground_truth_matchings = [\n",
    "    \"src:patient_cpr ‚âà tgt:person_id\",\n",
    "    \"src:laboratorium_idcode ‚âà tgt:provider_id\",\n",
    "    \"src:rekvirent_id ‚âà tgt:visit_occurence_id\",\n",
    "    \"src:rekvirent_id ‚âà tgt:visit_detail_id\",\n",
    "    \"src:samplingdate ‚âà tgt:measurement_date\",\n",
    "    \"src:samplingdate ‚âà tgt:measurement_datetime\",\n",
    "    \"src:samplingtime ‚âà tgt:measurement_datetime\",\n",
    "    \"src:samplingtime ‚âà tgt:measurement_time\",\n",
    "    \"src:analysiscode ‚âà tgt:measurement_source_concept_id\",\n",
    "    \"src:analysiscode ‚âà tgt:measurement_concept_id\",\n",
    "    \"src:unit ‚âà tgt:unit_source_value\",\n",
    "    \"src:unit ‚âà tgt:unit_concept_id\",\n",
    "    \"src:value ‚âà tgt:measurement_source_value\",\n",
    "    \"src:value ‚âà tgt:value_as_concept_id\",\n",
    "    \"src:value ‚âà tgt:value_as_number\",\n",
    "    \"src:operator ‚âà tgt:operator_concept_id\",\n",
    "    \"src:referenceinterval_lowerlimit ‚âà tgt:range_low\",\n",
    "    \"src:referenceinterval_upperlimit ‚âà tgt:range_high\",\n",
    "    \"src:resultvalidation ‚âà tgt:value_as_concept_id\",\n",
    "    \"src:rekvirent_idtype ‚âà tgt:measurement_type_concept_id\",\n",
    "    \"src:resulttype ‚âà ‚àÖ\",\n",
    "    \"src:Measurement ‚âà tgt:Measurement\"\n",
    "]\n",
    "\n",
    "matchings_response = [\n",
    "\"src:Measurement ‚âà tgt:Measurement\",\n",
    "\"src:samplingtime ‚âà tgt:measurement_time\",\n",
    "\"src:samplingtime ‚âà tgt:measurement_datetime\",\n",
    "\"src:patient_cpr ‚âà tgt:person_id\",\n",
    "\"src:analysiscode ‚âà tgt:measurement_source_concept_id\",\n",
    "\"src:analysiscode ‚âà tgt:measurement_source_value\",\n",
    "\"src:laboratorium_idcode ‚âà tgt:provider_id\",\n",
    "\"src:referenceinterval_lowerlimit ‚âà tgt:range_low\",\n",
    "\"src:referenceinterval_upperlimit ‚âà tgt:range_high\",\n",
    "\"src:unit ‚âà tgt:unit_source_value\",\n",
    "\"src:samplingdate ‚âà tgt:measurement_date\",\n",
    "\"src:samplingdate ‚âà tgt:measurement_datetime\",\n",
    "\"src:value ‚âà tgt:measurement_source_value\",\n",
    "\"src:value ‚âà tgt:value_as_number\",\n",
    "\"src:value ‚âà tgt:value_as_concept_id\",\n",
    "\"src:operator ‚âà tgt:operator_concept_id\"\n",
    "]\n",
    "\n",
    "gt_matchings_curated = [\n",
    "\"src:Measurement ‚âà tgt:Measurement\",\n",
    "\"src:laboratorium_idcode ‚âà tgt:provider_id\",\n",
    "\"src:rekvirent_id ‚âà tgt:visit_occurence_id\",\n",
    "\"src:rekvirent_id ‚âà tgt:visit_detail_id\",\n",
    "\"src:samplingdate ‚âà tgt:measurement_date\",\n",
    "\"src:samplingdate ‚âà tgt:measurement_datetime\",\n",
    "\"src:samplingtime ‚âà tgt:measurement_datetime\",\n",
    "\"src:samplingtime ‚âà tgt:measurement_time\",\n",
    "\"src:analysiscode ‚âà tgt:measurement_source_concept_id\",\n",
    "\"src:analysiscode ‚âà tgt:measurement_concept_id\",\n",
    "\"src:unit ‚âà tgt:unit_source_value\",\n",
    "\"src:unit ‚âà tgt:unit_concept_id\",\n",
    "\"src:value ‚âà tgt:value_as_number\",\n",
    "\"src:value ‚âà tgt:value_as_concept_id\",\n",
    "\"src:value ‚âà tgt:measurement_source_value\",\n",
    "\"src:operator ‚âà tgt:operator_concept_id\",\n",
    "\"src:referenceinterval_lowerlimit ‚âà tgt:range_low\",\n",
    "\"src:referenceinterval_upperlimit ‚âà tgt:range_high\",\n",
    "\"src:resultvalidation ‚âà tgt:value_as_concept_id\",\n",
    "\"src:rekvirent_idtype ‚âà tgt:measurement_type_concept_id\"]\n",
    "\n",
    "import ast\n",
    "import re\n",
    "\n",
    "def parse_llm_list_output(raw_text: str):\n",
    "    try:\n",
    "        # Step 1: Remove ```python and ``` (if present)\n",
    "        cleaned = raw_text.strip()\n",
    "        cleaned = re.sub(r\"^```python\\s*\", \"\", cleaned)\n",
    "        cleaned = re.sub(r\"\\s*```$\", \"\", cleaned)\n",
    "\n",
    "        # Step 2: Evaluate as literal Python\n",
    "        result = ast.literal_eval(cleaned)\n",
    "\n",
    "        # Step 3: Ensure list of strings\n",
    "        if isinstance(result, list):\n",
    "            return [item.strip() for item in result if isinstance(item, str)]\n",
    "        else:\n",
    "            raise ValueError(\"Parsed object is not a list.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing list: {e}\")\n",
    "        return []\n",
    "\n",
    "gpt4_matches = parse_llm_list_output(gpt4_response_match)\n",
    "\n",
    "print(gpt4_matches)\n",
    "\n",
    "def evaluate_alignment(ground_truth_list, predicted_list):\n",
    "    ground_truth_set = set(map(str.strip, ground_truth_list))\n",
    "    predicted_set = set(map(str.strip, predicted_list))\n",
    "\n",
    "    true_positives = ground_truth_set & predicted_set\n",
    "    false_positives = predicted_set - ground_truth_set\n",
    "    false_negatives = ground_truth_set - predicted_set\n",
    "\n",
    "    precision = len(true_positives) / len(predicted_set) if predicted_set else 0\n",
    "    recall = len(true_positives) / len(ground_truth_set) if ground_truth_set else 0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) else 0\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1,\n",
    "        \"true_positives\": sorted(true_positives),\n",
    "        \"false_positives\": sorted(false_positives),\n",
    "        \"false_negatives\": sorted(false_negatives),\n",
    "    }\n",
    "\n",
    "result = evaluate_alignment(gt_matchings_curated, gpt4_matches)\n",
    "\n",
    "print(f\"Precision: {result['precision']:.2f}\")\n",
    "print(f\"Recall: {result['recall']:.2f}\")\n",
    "print(f\"F1 Score: {result['f1_score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce8b26a-d49f-4b77-bcbc-851d6c3ca77b",
   "metadata": {},
   "source": [
    "### Mappings Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "740709f8-a399-46ef-bb11-d62f2e1766f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_fol_rules = [\n",
    "    \"‚àÄx (src:Measurement(x) ‚Üí tgt:Measurement(x))\",\n",
    "    \"‚àÄx,c (src:patient_cpr(x,c) ‚Üí tgt:person_id(x, hash_to_int(c)))\",\n",
    "    \"‚àÄx,l (src:laboratorium_idcode(x,l) ‚Üí tgt:provider_id(x, hash_to_int(l)))\",\n",
    "    \"‚àÄx,t (src:samplingtime(x,t) ‚Üí tgt:measurement_time(x,t))\",\n",
    "    \"‚àÄx,d (src:samplingdate(x,d) ‚Üí tgt:measurement_date(x,d))\",\n",
    "    \"‚àÄx,d,t (src:samplingdate(x,d) ‚àß src:samplingtime(x,t) ‚Üí tgt:measurement_datetime(x, combine_date_time(d, t)))\",\n",
    "    \"‚àÄx,a (src:analysiscode(x,a) ‚Üí tgt:measurement_source_value(x,a))\",\n",
    "    \"‚àÄx,a (src:analysiscode(x,a) ‚Üí tgt:measurement_concept_id(x, lookup_measurement_concept(a)))\",\n",
    "    \"‚àÄx,u (src:unit(x,u) ‚Üí tgt:unit_source_value(x,u))\",\n",
    "    \"‚àÄx,u (src:unit(x,u) ‚Üí tgt:unit_concept_id(x, lookup_unit_concept(u)))\",\n",
    "    \"‚àÄx,v (src:value(x,v) ‚àß is_numeric(v) ‚Üí tgt:value_as_number(x, v))\",\n",
    "    \"‚àÄx,v (src:value(x,v) ‚àß is_categorical(v) ‚Üí tgt:value_as_concept_id(x, lookup_concept_id(v)))\",\n",
    "    \"‚àÄx,v (src:value(x,v) ‚Üí tgt:value_source_value(x,v))\",\n",
    "    \"‚àÄx,o (src:operator(x,o) ‚Üí tgt:operator_concept_id(x, lookup_operator_concept(o)))\",\n",
    "    \"‚àÄx,r (src:resultvalidation(x,r) ‚Üí tgt:value_as_concept_id(x, lookup_concept_id(r)))\",\n",
    "    \"‚àÄx,m (src:rekvirent_idtype(x,m) ‚Üí tgt:measurement_type_concept_id(x, lookup_concept_id(m)))\",\n",
    "    \"‚àÄx,lo (src:referenceinterval_lowerlimit(x,lo) ‚Üí tgt:range_low(x, lo))\",\n",
    "    \"‚àÄx,hi (src:referenceinterval_upperlimit(x,hi) ‚Üí tgt:range_high(x, hi))\"\n",
    "]\n",
    "\n",
    "ground_truth_fol_rules_curated = [\n",
    "    \"‚àÄx (src:Measurement(x) ‚Üí tgt:Measurement(x))\",\n",
    "    \"‚àÄx,p (src:patient_cpr(x,p) ‚Üí tgt:person_id(x,lookup_person_id(p)))\",\n",
    "    \"‚àÄx,l (src:laboratorium_idcode(x,l) ‚Üí tgt:provider_id(x,lookup_provider_id(l)))\",\n",
    "    \"‚àÄx,t (src:samplingtime(x,t) ‚Üí tgt:measurement_time(x,t))\",\n",
    "    \"‚àÄx,d (src:samplingdate(x,d) ‚Üí tgt:measurement_date(x,d))\",\n",
    "    \"‚àÄx,d,t,dt (src:samplingdate(x,d) ‚àß src:samplingtime(x,t) ‚àß combine_date_time(d,t,dt) ‚Üí tgt:measurement_datetime(x,dt))\",\n",
    "    \"‚àÄx,d (src:samplingdate(x,d) ‚àß src:samplingtime(x,t) ‚àß combine_date_time(d,t,dt) ‚Üí tgt:measurement_datetime(x,dt))\",\n",
    "    \"‚àÄx,c (src:analysiscode(x,c) ‚Üí tgt:measurement_source_value(x,c))\",\n",
    "    \"‚àÄx,a (src:analysiscode(x,a) ‚Üí tgt:measurement_source_concept_id(x,lookup_concept_id(a)))\",\n",
    "    \"‚àÄx,u (src:unit(x,u) ‚Üí tgt:unit_source_value(x,u))\",\n",
    "    \"‚àÄx,u (src:unit(x,u) ‚Üí tgt:unit_concept_id(x,lookup_unit_concept(u)))\",\n",
    "    \"‚àÄx,v (src:value(x,v) ‚Üí tgt:measurement_source_value(x,v))\",\n",
    "    \"‚àÄx,v (src:value(x,v) ‚àß is_numeric(v,n) ‚Üí tgt:value_as_number(x,to_decimal(n)))\",\n",
    "    \"‚àÄx,v (src:value(x,v) ‚àß ¬¨is_numeric(v,_) ‚Üí tgt:value_as_concept_id(x,lookup_concept_id(v)))\",\n",
    "    \"‚àÄx,o (src:operator(x,o) ‚Üí tgt:operator_concept_id(x,lookup_operator_concept(o)))\",\n",
    "    \"‚àÄx,r (src:resultvalidation(x,r) ‚Üí tgt:value_as_concept_id(x, lookup_concept_id(r)))\",\n",
    "    \"‚àÄx,m (src:rekvirent_idtype(x,m) ‚Üí tgt:measurement_type_concept_id(x, lookup_concept_id(m)))\",\n",
    "    \"‚àÄx,low (src:referenceinterval_lowerlimit(x,low) ‚Üí tgt:range_low(x,to_decimal(low)))\",\n",
    "    \"‚àÄx,high (src:referenceinterval_upperlimit(x,high) ‚Üí tgt:range_high(x,to_decimal(high)))\"\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1abe34e9-404c-442f-8804-11f17d320471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚àÄx (src:Measurement(x) ‚Üí tgt:Measurement(x))\n",
      "\n",
      "‚àÄx,t (src:samplingtime(x,t) ‚Üí tgt:measurement_time(x,t))\n",
      "\n",
      "‚àÄx,t (src:samplingtime(x,t) ‚àß src:samplingdate(x,d) ‚àß combine_date_time(d,t,dt) ‚Üí tgt:measurement_datetime(x,dt))\n",
      "\n",
      "‚àÄx,d (src:samplingdate(x,d) ‚Üí tgt:measurement_date(x,d))\n",
      "\n",
      "‚àÄx,d (src:samplingdate(x,d) ‚àß src:samplingtime(x,t) ‚àß combine_date_time(d,t,dt) ‚Üí tgt:measurement_datetime(x,dt))\n",
      "\n",
      "‚àÄx,p (src:patient_cpr(x,p) ‚Üí tgt:person_id(x,lookup_person_id(p)))\n",
      "\n",
      "‚àÄx,a (src:analysiscode(x,a) ‚Üí tgt:measurement_source_concept_id(x,lookup_concept_id(a)))\n",
      "\n",
      "‚àÄx,a (src:analysiscode(x,a) ‚Üí tgt:measurement_source_value(x,a))\n",
      "\n",
      "‚àÄx,l (src:laboratorium_idcode(x,l) ‚Üí tgt:provider_id(x,lookup_provider_id(l)))\n",
      "\n",
      "‚àÄx,low (src:referenceinterval_lowerlimit(x,low) ‚Üí tgt:range_low(x,to_decimal(low)))\n",
      "\n",
      "‚àÄx,high (src:referenceinterval_upperlimit(x,high) ‚Üí tgt:range_high(x,to_decimal(high)))\n",
      "\n",
      "‚àÄx,u (src:unit(x,u) ‚Üí tgt:unit_source_value(x,u))\n",
      "\n",
      "‚àÄx,u (src:unit(x,u) ‚Üí tgt:unit_concept_id(x,lookup_unit_concept(u)))\n",
      "\n",
      "‚àÄx,v (src:value(x,v) ‚Üí tgt:measurement_source_value(x,v))\n",
      "\n",
      "‚àÄx,v (src:value(x,v) ‚àß is_numeric(v,n) ‚Üí tgt:value_as_number(x,to_decimal(n)))\n",
      "\n",
      "‚àÄx,v (src:value(x,v) ‚àß ¬¨is_numeric(v,_) ‚Üí tgt:value_as_concept_id(x,lookup_concept_id(v)))\n",
      "\n",
      "‚àÄx,o (src:operator(x,o) ‚Üí tgt:operator_concept_id(x,lookup_operator_concept(o)))\n"
     ]
    }
   ],
   "source": [
    "print(gpt4_response_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2756015-99b0-46bf-aa2b-e63047012c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.94\n",
      "Recall: 0.84\n",
      "F1 Score: 0.89\n"
     ]
    }
   ],
   "source": [
    "# 1. Define Equivalence Mappings\n",
    "function_equivalents = {\n",
    "    \"lookup_operator_concept\": \"lookup_operator_concept\",\n",
    "    \"lookup_concept_id\": \"lookup_measurement_concept\",\n",
    "    \"lookup_person_id\": \"hash_to_int\",\n",
    "    \"lookup_provider_id\": \"hash_to_int\",\n",
    "    \"parse_decimal\": None  # Assume it‚Äôs acceptable noise\n",
    "}\n",
    "\n",
    "predicate_equivalents = {\n",
    "    #\"measurement_source_concept_id\": \"measurement_source_value\",\n",
    "    \"value_as_concept_id\": \"value_as_concept_id\",\n",
    "    \"person_id\": \"person_id\",  # redundant but keeps format consistent\n",
    "}\n",
    "\n",
    "# 2. Normalize Rules\n",
    "import re\n",
    "\n",
    "def normalize_fol_rule(rule, func_map, pred_map):\n",
    "    # 1. Remove whitespace redundancy\n",
    "    rule = re.sub(r'\\s+', ' ', rule.strip())\n",
    "\n",
    "    # 2. Substitute helper functions\n",
    "    for model_func, canon_func in func_map.items():\n",
    "        if canon_func:\n",
    "            rule = re.sub(rf'\\b{model_func}\\b', canon_func, rule)\n",
    "        else:\n",
    "            # Strip functions like parse_decimal(x) ‚Üí x\n",
    "            rule = re.sub(rf'{model_func}\\(([^)]+)\\)', r'\\1', rule)\n",
    "\n",
    "    # 3. Substitute predicates\n",
    "    for model_pred, canon_pred in pred_map.items():\n",
    "        rule = re.sub(rf'\\btgt:{model_pred}\\b', f'tgt:{canon_pred}', rule)\n",
    "        rule = re.sub(rf'\\bsrc:{model_pred}\\b', f'src:{canon_pred}', rule)\n",
    "\n",
    "    # 4. Normalize variable names\n",
    "    var_match = re.match(r'^‚àÄ([^\\(]+)\\(', rule)\n",
    "    if var_match:\n",
    "        # Extract original variable list\n",
    "        var_list = [v.strip() for v in var_match.group(1).split(',')]\n",
    "        var_map = {old: f'x{i+1}' for i, old in enumerate(var_list)}\n",
    "\n",
    "        # Replace variables consistently in the rule\n",
    "        for old, new in var_map.items():\n",
    "            rule = re.sub(rf'\\b{old}\\b', new, rule)\n",
    "\n",
    "    return rule\n",
    "\n",
    "\n",
    "def evaluate_normalized_rules(gt_rules, llm_rules, func_map, pred_map):\n",
    "    norm_gt = set(normalize_fol_rule(r, func_map, pred_map) for r in gt_rules)\n",
    "    norm_llm = set(normalize_fol_rule(r, func_map, pred_map) for r in llm_rules)\n",
    "\n",
    "    true_positives = norm_gt & norm_llm\n",
    "    false_positives = norm_llm - norm_gt\n",
    "    false_negatives = norm_gt - norm_llm\n",
    "\n",
    "    precision = len(true_positives) / len(norm_llm) if norm_llm else 0\n",
    "    recall = len(true_positives) / len(norm_gt) if norm_gt else 0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) else 0\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1,\n",
    "        \"true_positives\": sorted(true_positives),\n",
    "        \"false_positives\": sorted(false_positives),\n",
    "        \"false_negatives\": sorted(false_negatives)\n",
    "    }\n",
    "\n",
    "llm_rules = [r.strip() for r in gpt4_response_map.split(\"\\n\") if r.strip()]\n",
    "results = evaluate_normalized_rules(ground_truth_fol_rules_curated, llm_rules, function_equivalents, predicate_equivalents)\n",
    "\n",
    "print(f\"Precision: {results['precision']:.2f}\")\n",
    "print(f\"Recall: {results['recall']:.2f}\")\n",
    "print(f\"F1 Score: {results['f1_score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801fcac2-045f-4033-82ab-bf388a3f3b8b",
   "metadata": {},
   "source": [
    "### Evaluate FGF Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3193d972-3dc5-41b2-a975-9ceca7ede8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FGF function extracted and loaded.\n"
     ]
    }
   ],
   "source": [
    "# 1. Clean and extract the Python code from gpt4_response_fgf\n",
    "def extract_and_define_fgf(gpt4_response_fgf: str):\n",
    "    # Remove ```python ... ``` if present\n",
    "    cleaned_code = re.sub(r\"^```python\\n?\", \"\", gpt4_response_fgf)\n",
    "    cleaned_code = re.sub(r\"\\n?```$\", \"\", cleaned_code)\n",
    "\n",
    "    # 2. Define a local namespace for safe execution\n",
    "    local_namespace = {}\n",
    "\n",
    "    # 3. Execute the cleaned code in the local namespace\n",
    "    exec(cleaned_code, globals(), local_namespace)\n",
    "\n",
    "    # 4. Return the extracted transform_measurement function\n",
    "    return local_namespace.get(\"transform_measurement\", None)\n",
    "\n",
    "# Example usage:\n",
    "transform_measurement = extract_and_define_fgf(gpt4_response_fgf)\n",
    "\n",
    "# Check if it worked\n",
    "if transform_measurement:\n",
    "    print(\"‚úÖ FGF function extracted and loaded.\")\n",
    "else:\n",
    "    print(\"‚ùå FGF function not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7d8c7a7-d9bb-40a3-ac8e-1df998c72fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function transform_measurement at 0x00000217CF6E3010>\n"
     ]
    }
   ],
   "source": [
    "print(transform_measurement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4bdb25f5-f0eb-4875-83c7-3157d6e43bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with missing rdf:type src:Measurement statements\n",
    "\n",
    "from rdflib import Graph, URIRef, RDF, Namespace\n",
    "\n",
    "# Load the source graph\n",
    "src_graph = Graph()\n",
    "src_graph.parse(\"src_data_graph_new.nt\", format=\"nt\")\n",
    "\n",
    "# Define the namespace\n",
    "SRC = Namespace(\"http://example.org/src#\")\n",
    "\n",
    "# Find all unique measurement subjects\n",
    "measurement_subjects = set(src_graph.subjects())\n",
    "\n",
    "# Create a new graph for type declarations\n",
    "type_graph = Graph()\n",
    "\n",
    "# Add rdf:type triples for each subject\n",
    "for subj in measurement_subjects:\n",
    "    type_graph.add((subj, RDF.type, SRC.Measurement))\n",
    "\n",
    "src_graph += type_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f182784b-8eb7-4536-bc6e-70aa485114a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3002529\n",
      "<class 'rdflib.term.Literal'>\n"
     ]
    }
   ],
   "source": [
    "def lookup_concept_id(val_lit):\n",
    "    \"\"\"\n",
    "    Map a categorical value (rdflib term or plain str) to a\n",
    "    measurement_concept_id and return it as Literal(xsd:integer).\n",
    "    Returns None when no mapping is known.\n",
    "    \"\"\"\n",
    "    val = str(val_lit).strip()\n",
    "\n",
    "\n",
    "    # 1Ô∏è‚É£ direct table lookup\n",
    "    cid = SOURCE_CODE_TO_CONCEPT_ID.get(val)\n",
    "    print(cid)\n",
    "    if cid is not None:\n",
    "        return Literal(int(cid), datatype=XSD.integer)\n",
    "\n",
    "    # 2Ô∏è‚É£ heuristic prefixes\n",
    "    if val.lower().startswith((\"pos\")):\n",
    "        return Literal(45884084, datatype=XSD.integer)   # positive\n",
    "    if val.lower().startswith((\"neg\")):\n",
    "        return Literal(45878583, datatype=XSD.integer)   # negative\n",
    "    if val.lower().startswith((\"normal\")):\n",
    "        return Literal(45884153, datatype=XSD.integer)   # normal\n",
    "\n",
    "    # 3Ô∏è‚É£ special VIST rule\n",
    "    if \"VIST\" in val:\n",
    "        cid = 45877985 if val.startswith(\"P\") else 45880296\n",
    "        return Literal(cid, datatype=XSD.integer)\n",
    "\n",
    "    # üîö no match\n",
    "    return None\n",
    "\n",
    "print(type(lookup_concept_id('NPU26678')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2807f69e-dec8-45b2-9b4d-98a2beca47d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "484\n"
     ]
    }
   ],
   "source": [
    "#Step 1: Set up RDFLib and Namespaces\n",
    "from rdflib import Graph, Namespace, Literal, URIRef\n",
    "from rdflib.namespace import RDF, XSD\n",
    "\n",
    "SRC = Namespace(\"http://example.org/src#\")\n",
    "TGT = Namespace(\"http://example.org/tgt#\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load lookup tables\n",
    "cpr_df = pd.read_csv(\"cpr_to_person_id.csv\")\n",
    "labid_df = pd.read_csv(\"labid_to_provider_id.csv\")\n",
    "conversion_df = pd.read_csv(\"npu_loinc_conversion_table_expanded.csv\")\n",
    "\n",
    "# Step 2: Define the lookup and helper functions\n",
    "\n",
    "def parse_decimal(value):\n",
    "    try:\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def lookup_person_id(cpr):\n",
    "    key = str(cpr).strip().lower()               # normalise incoming CPR\n",
    "\n",
    "    # do the comparison on a normalised helper column\n",
    "    if \"cpr_lc\" not in cpr_df.columns:\n",
    "        cpr_df[\"cpr_lc\"] = cpr_df[\"cpr\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "    result = cpr_df.loc[cpr_df[\"cpr_lc\"] == key, \"person_id\"].dropna()\n",
    "\n",
    "    if not result.empty:\n",
    "        try:\n",
    "            return Literal(int(float(result.values[0])), datatype=XSD.integer)\n",
    "        except ValueError:\n",
    "            pass                                  # person_id is not numeric\n",
    "\n",
    "    return None\n",
    "\n",
    "def lookup_provider_id(rekvirent_id):\n",
    "    \"\"\"\n",
    "    Map `rekvirent_id` to provider_id and return it as\n",
    "    Literal(xsd:integer).  Returns None when no mapping exists.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    rekvirent_id : rdflib term or str\n",
    "        The lab-ID value coming from the source graph.\n",
    "    df : pandas.DataFrame\n",
    "        Dataframe with columns 'labid_value' and 'provider_id'.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> lookup_provider_id(Literal(\" LAB123 \"))\n",
    "    rdflib.term.Literal('789', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#integer'))\n",
    "    \"\"\"\n",
    "    df=labid_df\n",
    "    # 1Ô∏è‚É£ normalise the incoming key once\n",
    "    key = str(rekvirent_id).strip().lower()\n",
    "\n",
    "    # 2Ô∏è‚É£ create & reuse a normalised helper column on first call\n",
    "    if \"labid_value_lc\" not in df.columns:\n",
    "        df[\"labid_value_lc\"] = (\n",
    "            df[\"labid_value\"].astype(str).str.strip().str.lower()\n",
    "        )\n",
    "\n",
    "    matches = df.loc[df[\"labid_value_lc\"] == key, \"provider_id\"].dropna()\n",
    "\n",
    "    if not matches.empty:\n",
    "        try:\n",
    "            pid = int(float(matches.iloc[0]))          # handles \"123.0\" ‚Üí 123\n",
    "            return Literal(pid, datatype=XSD.integer)\n",
    "        except ValueError:\n",
    "            # provider_id is not numeric; fall through to 'None'\n",
    "            pass\n",
    "\n",
    "    return None\n",
    "\n",
    "from rdflib import Literal\n",
    "from rdflib.namespace import XSD\n",
    "from datetime import datetime\n",
    "\n",
    "def combine_date_time(date_literal, time_literal):\n",
    "    try:\n",
    "        # Convert RDFLib Literals to Python date and time strings\n",
    "        date_str = str(date_literal)\n",
    "        time_str = str(time_literal)\n",
    "\n",
    "        # Combine into a datetime object\n",
    "        dt = datetime.fromisoformat(f\"{date_str}T{time_str}\")\n",
    "\n",
    "        # Return as xsd:dateTime formatted string\n",
    "        return dt.isoformat()\n",
    "    except Exception as e:\n",
    "        # Return None if formatting fails\n",
    "        return None\n",
    "\n",
    "# Mapping from sourceCode to measurement_concept_id\n",
    "SOURCE_CODE_TO_CONCEPT_ID = {\n",
    "    \"NPU01682\": 3013466,\n",
    "    \"NPU20197\": 3002582,\n",
    "    \"NPU20198\": 3018095,\n",
    "    \"NPU02070\": 3021960,\n",
    "    \"NPU01435\": 40771025,\n",
    "    \"NPU19917\": 3020924,\n",
    "    \"NPU27591\": 3021337,\n",
    "    \"NPU01992\": 3020191,\n",
    "    \"NPU26678\": 3002529,\n",
    "    \"NPU01700\": 3009035,\n",
    "    \"NPU16065\": 3023421,\n",
    "    \"NPU03429\": 3019550,\n",
    "    \"NPU03577\": 3009201,\n",
    "    \"NPU02319\": 40762351,\n",
    "    \"NPU19661\": 3034485,\n",
    "    \"NPU27412\": 3013826,\n",
    "    \"NPU03230\": 3023103,\n",
    "    \"NPU12033\": 3017143,\n",
    "    \"DNK35302\": 46236975,\n",
    "    \"NPU19651\": 3006923,\n",
    "    \"NPU01944\": 3024731,\n",
    "    \"DNK35312\": 1259611,\n",
    "    \"NPU19748\": 3020460,\n",
    "    \"DNK35131\": 46236975,\n",
    "    \"NPU18016\": 3020564,\n",
    "    \"NPU19673\": 3024561,\n",
    "    \"NPU01568\": 3001308,\n",
    "    \"NPU01349\": 3006315,\n",
    "    \"NPU27783\": 3035995\n",
    "}\n",
    "\n",
    "SOURCE_UNIT_TO_UNIT_ID = {\n",
    "    \"s\": 8555.0,\n",
    "    \"10^6/l\": 9442.0,\n",
    "    \"nmol/L\": 8736.0,\n",
    "    \"ng/L\": 8725.0,\n",
    "    \"IU/L\": 9254.0,\n",
    "    \"mg/g\": 9364.0,\n",
    "    \"mmol/L\": 8753.0,\n",
    "    \"U/L\": 9442.0,\n",
    "    \"mol/L\": 8729.0,\n",
    "    \"mL/min\": 8795.0,  \n",
    "}\n",
    "\n",
    "def lookup_measurement_source_concept_id(code_lit):\n",
    "    \"\"\"\n",
    "    Given an rdflib Literal for the analysis code (sourceCode),\n",
    "    return the associated measurement_concept_id as an int,\n",
    "    or None if not found.\n",
    "    \"\"\"\n",
    "    code_str = str(code_lit)\n",
    "    return SOURCE_CODE_TO_CONCEPT_ID.get(code_str)\n",
    "\n",
    "def lookup_unit_concept(unit_lit):\n",
    "    \"\"\"\n",
    "    Map a unit label (rdflib term or plain str) to the corresponding\n",
    "    OMOP unit_concept_id.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    rdflib.Literal\n",
    "        ‚Ä¢ Literal(<concept_id>, xsd:integer)  when a mapping exists\n",
    "        ‚Ä¢ Literal(\"nan\", xsd:string)          when no mapping is found\n",
    "          (choose a different fallback if you prefer)\n",
    "    \"\"\"\n",
    "    # 1Ô∏è‚É£ canonicalise the key: string, trimmed, upper-case\n",
    "    key = str(unit_lit).strip().upper()\n",
    "\n",
    "    # 2Ô∏è‚É£ look it up in your mapping table\n",
    "    cid = SOURCE_UNIT_TO_UNIT_ID.get(key)\n",
    "\n",
    "    if cid is not None:\n",
    "        try:\n",
    "            return Literal(int(cid), datatype=XSD.integer)\n",
    "        except (ValueError, TypeError):\n",
    "            # cid wasn‚Äôt numeric; fall through to the fallback literal\n",
    "            pass\n",
    "\n",
    "    # 3Ô∏è‚É£ fallback literal so Graph.add() still receives a valid Node\n",
    "    return Literal(\"nan\", datatype=XSD.string)\n",
    "\n",
    "def parse_numeric(val_lit):\n",
    "    \"\"\"\n",
    "    Given an rdflib Literal containing a string representation of a number,\n",
    "    return it as an int or float. Returns None if parsing fails.\n",
    "    \"\"\"\n",
    "    val_str = str(val_lit).strip()\n",
    "    try:\n",
    "        if '.' in val_str:\n",
    "            return float(val_str)\n",
    "        else:\n",
    "            return int(val_str)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "CATEGORICAL_VALUE_TO_CONCEPT_ID = {\n",
    "    # Positive (case-insensitive startswith)\n",
    "    \"POS\": 45884084,\n",
    "    \"Pos\": 45884084,\n",
    "    \"pos\": 45884084,\n",
    "\n",
    "    # Negative (case-insensitive startswith)\n",
    "    \"NEG\": 45878583,\n",
    "    \"Neg\": 45878583,\n",
    "    \"neg\": 45878583,\n",
    "\n",
    "    # Normal (case-insensitive startswith)\n",
    "    \"NORMAL\": 45884153,\n",
    "    \"Normal\": 45884153,\n",
    "    \"normal\": 45884153,\n",
    "\n",
    "    # Exact matches\n",
    "    \"V√ÜKST\": 36032835,\n",
    "    \"INGEN V√ÜKST\": 42530718,\n",
    "    \"FORH√òJET\": 1620380,\n",
    "\n",
    "    # VIST handling is dynamic, handled in logic\n",
    "}\n",
    "\n",
    "def parse_categorical(val_lit):\n",
    "    \"\"\"\n",
    "    Determines if a value is categorical by checking the known dictionary or special patterns.\n",
    "    \"\"\"\n",
    "    val = str(val_lit).strip()\n",
    "\n",
    "    # Check exact match\n",
    "    if val in CATEGORICAL_VALUE_TO_CONCEPT_ID:\n",
    "        return True\n",
    "\n",
    "    # Check startswith patterns (POS, NEG, NORMAL)\n",
    "    if any(val.startswith(prefix) for prefix in [\"POS\", \"Pos\", \"pos\", \"NEG\", \"Neg\", \"neg\", \"NORMAL\", \"Normal\", \"normal\"]):\n",
    "        return True\n",
    "\n",
    "    # Check special VIST case\n",
    "    if \"VIST\" in val:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def lookup_concept_id(val_lit):\n",
    "    \"\"\"\n",
    "    Map a categorical value (rdflib term or plain str) to a\n",
    "    measurement_concept_id Literal(xsd:integer).  Returns None if no mapping.\n",
    "    \"\"\"\n",
    "\n",
    "    # ‚ë† make it a clean, comparable string\n",
    "    val = str(val_lit).strip()          # works for Literal, URIRef, str\n",
    "    low = val.lower()                   # for case-insensitive tests\n",
    "\n",
    "    # ‚ë° exact match on a *normalised* key (e.g. uppercase)\n",
    "    cid = SOURCE_CODE_TO_CONCEPT_ID.get(val) or \\\n",
    "          SOURCE_CODE_TO_CONCEPT_ID.get(val.upper()) or \\\n",
    "          SOURCE_CODE_TO_CONCEPT_ID.get(low)\n",
    "    if cid is not None:\n",
    "        return Literal(int(cid), datatype=XSD.integer)\n",
    "\n",
    "    # ‚ë¢ heuristic prefixes (now guaranteed to be a string)\n",
    "    if low.startswith(\"pos\"):\n",
    "        return Literal(45884084, datatype=XSD.integer)   # positive\n",
    "    if low.startswith(\"neg\"):\n",
    "        return Literal(45878583, datatype=XSD.integer)   # negative\n",
    "    if low.startswith(\"normal\"):\n",
    "        return Literal(45884153, datatype=XSD.integer)   # normal\n",
    "\n",
    "    # ‚ë£ special VIST rule\n",
    "    if \"VIST\" in val:\n",
    "        cid = 45877985 if val.startswith(\"P\") else 45880296\n",
    "        return Literal(cid, datatype=XSD.integer)\n",
    "\n",
    "    # ‚ë§ **fallback** ‚Äì no mapping\n",
    "    return Literal(\"nan\", datatype=XSD.string)\n",
    "\n",
    "\n",
    "OPERATOR_TEXT_TO_CONCEPT_ID = {\n",
    "    \"mindre_end\": 4171756,   # \"<\" (less than)\n",
    "    \"stoerre_end\": 4172704,  # \">\" (greater than)\n",
    "}\n",
    "\n",
    "from rdflib import Literal, XSD\n",
    "\n",
    "def lookup_operator_concept(op_lit):\n",
    "    \"\"\"\n",
    "    Map a source operator label (rdflib term or str) to its\n",
    "    OMOP operator_concept_id and return it as Literal(xsd:integer).\n",
    "\n",
    "    When the text is not recognised, the function returns\n",
    "    Literal(\"nan\", xsd:string) so the caller still gets a valid Node.\n",
    "    \"\"\"\n",
    "\n",
    "    # ‚ë† canonicalise input: string, trimmed, upper-case\n",
    "    key = str(op_lit).strip().upper()\n",
    "\n",
    "    # ‚ë° try the exact mapping\n",
    "    cid = OPERATOR_TEXT_TO_CONCEPT_ID.get(key)\n",
    "    if cid is not None:\n",
    "        try:\n",
    "            return Literal(int(cid), datatype=XSD.integer)\n",
    "        except (ValueError, TypeError):\n",
    "            # mapping exists but isn't numeric ‚Äì fall through to fallback\n",
    "            pass\n",
    "\n",
    "    # ‚ë¢ fallback literal keeps rdflib happy\n",
    "    return Literal(\"nan\", datatype=XSD.string)\n",
    "\n",
    "\n",
    "#Step 3: Paste the FGF function (transform_measurement)\n",
    "#Use the previously extracted FGF function and update it to use the above helpers\n",
    "\n",
    "from rdflib import Namespace, RDF, URIRef, Literal\n",
    "from rdflib.namespace import XSD\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Namespaces (customise the base URIs so they match your actual ontologies)\n",
    "# ---------------------------------------------------------------------------\n",
    "SRC = Namespace(\"http://example.org/src#\")\n",
    "TGT = Namespace(\"http://example.org/tgt#\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Helper --------------------------------------------------------------------\n",
    "# ---------------------------------------------------------------------------\n",
    "def _local_name(uri, namespace):\n",
    "    \"\"\"\n",
    "    Return the local part of `uri` relative to `namespace`.\n",
    "    If the URI is outside the namespace, fall back to the last path fragment.\n",
    "    \"\"\"\n",
    "    ns_str = str(namespace)\n",
    "    uri_str = str(uri)\n",
    "    return uri_str[len(ns_str):] if uri_str.startswith(ns_str) else uri_str.rsplit(\"/\", 1)[-1]\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Main Fact-Generating Function ---------------------------------------------\n",
    "# ---------------------------------------------------------------------------\n",
    "from rdflib import URIRef, Literal, RDF, XSD\n",
    "from rdflib.namespace import Namespace\n",
    "from datetime import datetime, date, time\n",
    "\n",
    "SRC = Namespace(\"http://example.org/src#\")\n",
    "TGT = Namespace(\"http://example.org/tgt#\")\n",
    "\n",
    "def combine_date_time(date_val, time_val):\n",
    "    \"\"\"Combine xsd:date and xsd:time Literal into xsd:dateTime string.\"\"\"\n",
    "    if not (isinstance(date_val, Literal) and isinstance(time_val, Literal)):\n",
    "        return None\n",
    "    try:\n",
    "        dt = datetime.combine(date.fromisoformat(str(date_val)), time.fromisoformat(str(time_val)))\n",
    "        return Literal(dt.isoformat(), datatype=XSD.dateTime)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def to_decimal(val):\n",
    "    \"\"\"Converts a value which can be string, int etc. to a decimal-formatted Literal.\"\"\"\n",
    "    try:\n",
    "        return Literal(float(val), datatype=XSD.decimal)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def is_numeric(val):\n",
    "    \"\"\"Checks if given value (Literal) is numeric; if so, returns number as string, else None.\"\"\"\n",
    "    try:\n",
    "        n = float(str(val))\n",
    "        return True, str(n)\n",
    "    except Exception:\n",
    "        return False, None\n",
    "\n",
    "# this is the generated FGF\n",
    "def transform_measurement(src_graph, tgt_graph):\n",
    "    \"\"\"\n",
    "    FGF for src:Measurement aligned to tgt:Measurement.\n",
    "    Transfers mapped properties with required transformation.\n",
    "    \"\"\"\n",
    "    for meas in src_graph.subjects(RDF.type, SRC.Measurement):\n",
    "        tgt_graph.add((meas, RDF.type, TGT.Measurement))\n",
    "        \n",
    "        # src:samplingtime(x,t) ‚Üí tgt:measurement_time(x,t)\n",
    "        for _, _, t in src_graph.triples((meas, SRC.samplingtime, None)):\n",
    "            tgt_graph.add((meas, TGT.measurement_time, Literal(t, datatype=XSD.time)))\n",
    "\n",
    "        # src:samplingdate(x,d) ‚Üí tgt:measurement_date(x,d)\n",
    "        for _, _, d in src_graph.triples((meas, SRC.samplingdate, None)):\n",
    "            tgt_graph.add((meas, TGT.measurement_date, Literal(d, datatype=XSD.date)))\n",
    "        \n",
    "        # src:samplingdate + src:samplingtime ‚Üí tgt:measurement_datetime(x,dt)\n",
    "        d = next((o for _, _, o in src_graph.triples((meas, SRC.samplingdate, None))), None)\n",
    "        t = next((o for _, _, o in src_graph.triples((meas, SRC.samplingtime, None))), None)\n",
    "        if d is not None and t is not None:\n",
    "            dt = combine_date_time(d, t)\n",
    "            if dt is not None:\n",
    "                tgt_graph.add((meas, TGT.measurement_datetime, dt))\n",
    "\n",
    "        # src:patient_cpr(x,p) ‚Üí tgt:person_id(x,lookup_person_id(p))\n",
    "        for _, _, p in src_graph.triples((meas, SRC.patient_cpr, None)):\n",
    "            tgt_graph.add((meas, TGT.person_id, lookup_person_id(p)))\n",
    "        \n",
    "        # src:analysiscode(x,a) ‚Üí tgt:measurement_source_concept_id(x,lookup_concept_id(a)), tgt:measurement_source_value(x,a)\n",
    "        for _, _, a in src_graph.triples((meas, SRC.analysiscode, None)):\n",
    "            tgt_graph.add((meas, TGT.measurement_source_concept_id, lookup_concept_id(a)))\n",
    "            tgt_graph.add((meas, TGT.measurement_source_value, Literal(a, datatype=XSD.string)))\n",
    "\n",
    "        # src:laboratorium_idcode(x,l) ‚Üí tgt:provider_id(x,lookup_provider_id(l))\n",
    "        for _, _, l in src_graph.triples((meas, SRC.laboratorium_idcode, None)):\n",
    "            tgt_graph.add((meas, TGT.provider_id, lookup_provider_id(l)))\n",
    "        \n",
    "        # src:referenceinterval_lowerlimit(x,low) ‚Üí tgt:range_low(x,to_decimal(low))\n",
    "        for _, _, low in src_graph.triples((meas, SRC.referenceinterval_lowerlimit, None)):\n",
    "            d = to_decimal(low)\n",
    "            if d is not None:\n",
    "                tgt_graph.add((meas, TGT.range_low, d))\n",
    "        \n",
    "        # src:referenceinterval_upperlimit(x,high) ‚Üí tgt:range_high(x,to_decimal(high))\n",
    "        for _, _, high in src_graph.triples((meas, SRC.referenceinterval_upperlimit, None)):\n",
    "            d = to_decimal(high)\n",
    "            if d is not None:\n",
    "                tgt_graph.add((meas, TGT.range_high, d))\n",
    "        \n",
    "        # src:unit(x,u) ‚Üí tgt:unit_source_value(x,u), tgt:unit_concept_id(x,lookup_unit_concept(u))\n",
    "        for _, _, u in src_graph.triples((meas, SRC.unit, None)):\n",
    "            tgt_graph.add((meas, TGT.unit_source_value, Literal(u, datatype=XSD.string)))\n",
    "            tgt_graph.add((meas, TGT.unit_concept_id, lookup_unit_concept(u)))\n",
    "\n",
    "        # src:value(x,v) ‚Üí tgt:measurement_source_value(x,v)\n",
    "        for _, _, v in src_graph.triples((meas, SRC.value, None)):\n",
    "            tgt_graph.add((meas, TGT.measurement_source_value, Literal(v, datatype=XSD.string)))\n",
    "\n",
    "            # src:value(x,v) ‚àß is_numeric(v,n) ‚Üí tgt:value_as_number(x,to_decimal(n))\n",
    "            isnum, n = is_numeric(v)\n",
    "            if isnum:\n",
    "                num_lit = to_decimal(n)\n",
    "                if num_lit is not None:\n",
    "                    tgt_graph.add((meas, TGT.value_as_number, num_lit))\n",
    "            else:\n",
    "                # src:value(x,v) ‚àß ¬¨is_numeric(v,_) ‚Üí tgt:value_as_concept_id(x,lookup_concept_id(v))\n",
    "                tgt_graph.add((meas, TGT.value_as_concept_id, lookup_concept_id(v)))\n",
    "\n",
    "        # src:operator(x,o) ‚Üí tgt:operator_concept_id(x,lookup_operator_concept(o))\n",
    "        for _, _, o in src_graph.triples((meas, SRC.operator, None)):\n",
    "            tgt_graph.add((meas, TGT.operator_concept_id, lookup_operator_concept(o)))\n",
    "\n",
    "# Step 4: Generate the Target Graph\n",
    "# Create the target graph\n",
    "tgt_graph = Graph()\n",
    "\n",
    "# Run the FGF function\n",
    "transform_measurement(src_graph, tgt_graph)\n",
    "\n",
    "# Optional: Save the result\n",
    "#tgt_graph.serialize('openai_generated_target_graph.nt', format='nt')\n",
    "print(len(tgt_graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ccab812c-a5be-4281-a65e-9f2bec5f2732",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#integer, Converter=<class 'int'>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ramme\\anaconda3\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "ValueError: invalid literal for int() with base 10: '8753.0'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#integer, Converter=<class 'int'>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ramme\\anaconda3\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "ValueError: invalid literal for int() with base 10: '44777588.0'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#integer, Converter=<class 'int'>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ramme\\anaconda3\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "ValueError: invalid literal for int() with base 10: '9364.0'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#integer, Converter=<class 'int'>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ramme\\anaconda3\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "ValueError: invalid literal for int() with base 10: '8751.0'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#integer, Converter=<class 'int'>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ramme\\anaconda3\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "ValueError: invalid literal for int() with base 10: '45884084.0'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#integer, Converter=<class 'int'>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ramme\\anaconda3\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "ValueError: invalid literal for int() with base 10: 'nan'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#integer, Converter=<class 'int'>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ramme\\anaconda3\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "ValueError: invalid literal for int() with base 10: '8753.0'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#integer, Converter=<class 'int'>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ramme\\anaconda3\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "ValueError: invalid literal for int() with base 10: '45884084.0'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#integer, Converter=<class 'int'>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ramme\\anaconda3\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "ValueError: invalid literal for int() with base 10: '8753.0'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#integer, Converter=<class 'int'>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ramme\\anaconda3\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "ValueError: invalid literal for int() with base 10: 'nan'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#integer, Converter=<class 'int'>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ramme\\anaconda3\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "ValueError: invalid literal for int() with base 10: '8795.0'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#integer, Converter=<class 'int'>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ramme\\anaconda3\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "ValueError: invalid literal for int() with base 10: '8736.0'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#integer, Converter=<class 'int'>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ramme\\anaconda3\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "ValueError: invalid literal for int() with base 10: '8753.0'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#integer, Converter=<class 'int'>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ramme\\anaconda3\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "ValueError: invalid literal for int() with base 10: '8645.0'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#integer, Converter=<class 'int'>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ramme\\anaconda3\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "ValueError: invalid literal for int() with base 10: '9442.0'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#integer, Converter=<class 'int'>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ramme\\anaconda3\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "ValueError: invalid literal for int() with base 10: '8583.0'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#integer, Converter=<class 'int'>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ramme\\anaconda3\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "ValueError: invalid literal for int() with base 10: '9442.0'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#integer, Converter=<class 'int'>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ramme\\anaconda3\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "ValueError: invalid literal for int() with base 10: '8729.0'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#integer, Converter=<class 'int'>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ramme\\anaconda3\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "ValueError: invalid literal for int() with base 10: '8795.0'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#integer, Converter=<class 'int'>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ramme\\anaconda3\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "ValueError: invalid literal for int() with base 10: '8555.0'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#integer, Converter=<class 'int'>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ramme\\anaconda3\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "ValueError: invalid literal for int() with base 10: '9442.0'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#integer, Converter=<class 'int'>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ramme\\anaconda3\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "ValueError: invalid literal for int() with base 10: '8645.0'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#integer, Converter=<class 'int'>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ramme\\anaconda3\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "ValueError: invalid literal for int() with base 10: '9254.0'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#integer, Converter=<class 'int'>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ramme\\anaconda3\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "ValueError: invalid literal for int() with base 10: '8636.0'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#integer, Converter=<class 'int'>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ramme\\anaconda3\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "ValueError: invalid literal for int() with base 10: '8725.0'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#integer, Converter=<class 'int'>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ramme\\anaconda3\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "ValueError: invalid literal for int() with base 10: '8749.0'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#integer, Converter=<class 'int'>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ramme\\anaconda3\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "ValueError: invalid literal for int() with base 10: '44777561.0'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#integer, Converter=<class 'int'>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ramme\\anaconda3\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "ValueError: invalid literal for int() with base 10: '8736.0'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#integer, Converter=<class 'int'>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ramme\\anaconda3\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "ValueError: invalid literal for int() with base 10: '8753.0'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#integer, Converter=<class 'int'>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ramme\\anaconda3\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "ValueError: invalid literal for int() with base 10: '9442.0'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#integer, Converter=<class 'int'>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ramme\\anaconda3\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "ValueError: invalid literal for int() with base 10: '8751.0'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#integer, Converter=<class 'int'>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ramme\\anaconda3\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "ValueError: invalid literal for int() with base 10: '9442.0'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#integer, Converter=<class 'int'>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ramme\\anaconda3\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "ValueError: invalid literal for int() with base 10: '8751.0'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#integer, Converter=<class 'int'>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ramme\\anaconda3\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "ValueError: invalid literal for int() with base 10: '45884084.0'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#integer, Converter=<class 'int'>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ramme\\anaconda3\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "ValueError: invalid literal for int() with base 10: '8795.0'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#integer, Converter=<class 'int'>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ramme\\anaconda3\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "ValueError: invalid literal for int() with base 10: '8795.0'\n",
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#integer, Converter=<class 'int'>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ramme\\anaconda3\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "ValueError: invalid literal for int() with base 10: '8753.0'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicate</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://example.org/tgt#measurement_datetime</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.345455</td>\n",
       "      <td>0.513514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://example.org/tgt#provider_id</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.987013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://example.org/tgt#measurement_source_conc...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://example.org/tgt#measurement_source_value</td>\n",
       "      <td>38.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.660870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://example.org/tgt#unit_concept_id</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://example.org/tgt#measurement_id</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>http://example.org/tgt#unit_source_value</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>http://example.org/tgt#person_id</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.987013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>http://example.org/tgt#measurement_concept_id</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>http://example.org/tgt#measurement_time</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.987013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>http://www.w3.org/1999/02/22-rdf-syntax-ns#type</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.987013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>http://example.org/tgt#measurement_date</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.987013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>http://example.org/tgt#range_high</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>http://example.org/tgt#range_low</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>http://example.org/tgt#value_as_number</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>http://example.org/tgt#value_as_concept_id</td>\n",
       "      <td>4.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>http://example.org/tgt#measurement_type_concep...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>http://example.org/tgt#value_source_value</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>http://example.org/tgt#operator_concept_id</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>MICRO_AVG</td>\n",
       "      <td>295.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>319.0</td>\n",
       "      <td>0.609504</td>\n",
       "      <td>0.480456</td>\n",
       "      <td>0.537341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>MACRO_AVG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.406433</td>\n",
       "      <td>0.413593</td>\n",
       "      <td>0.391725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            predicate     TP     FP     FN  \\\n",
       "0         http://example.org/tgt#measurement_datetime   38.0    0.0   72.0   \n",
       "1                  http://example.org/tgt#provider_id   38.0    0.0    1.0   \n",
       "2   http://example.org/tgt#measurement_source_conc...    0.0   38.0    0.0   \n",
       "3     http://example.org/tgt#measurement_source_value   38.0   38.0    1.0   \n",
       "4              http://example.org/tgt#unit_concept_id    0.0   25.0   37.0   \n",
       "5               http://example.org/tgt#measurement_id    0.0    0.0   39.0   \n",
       "6            http://example.org/tgt#unit_source_value   25.0    0.0    0.0   \n",
       "7                    http://example.org/tgt#person_id   38.0    0.0    1.0   \n",
       "8       http://example.org/tgt#measurement_concept_id    0.0    0.0   37.0   \n",
       "9             http://example.org/tgt#measurement_time   38.0    0.0    1.0   \n",
       "10    http://www.w3.org/1999/02/22-rdf-syntax-ns#type   38.0    0.0    1.0   \n",
       "11            http://example.org/tgt#measurement_date   38.0    0.0    1.0   \n",
       "12                  http://example.org/tgt#range_high    0.0   24.0    0.0   \n",
       "13                   http://example.org/tgt#range_low    0.0   25.0    0.0   \n",
       "14             http://example.org/tgt#value_as_number    0.0   20.0   32.0   \n",
       "15         http://example.org/tgt#value_as_concept_id    4.0   14.0    2.0   \n",
       "16  http://example.org/tgt#measurement_type_concep...    0.0    0.0   39.0   \n",
       "17          http://example.org/tgt#value_source_value    0.0    0.0   39.0   \n",
       "18         http://example.org/tgt#operator_concept_id    0.0    5.0   16.0   \n",
       "19                                          MICRO_AVG  295.0  189.0  319.0   \n",
       "20                                          MACRO_AVG    NaN    NaN    NaN   \n",
       "\n",
       "    Precision    Recall        F1  \n",
       "0    1.000000  0.345455  0.513514  \n",
       "1    1.000000  0.974359  0.987013  \n",
       "2    0.000000  0.000000  0.000000  \n",
       "3    0.500000  0.974359  0.660870  \n",
       "4    0.000000  0.000000  0.000000  \n",
       "5    0.000000  0.000000  0.000000  \n",
       "6    1.000000  1.000000  1.000000  \n",
       "7    1.000000  0.974359  0.987013  \n",
       "8    0.000000  0.000000  0.000000  \n",
       "9    1.000000  0.974359  0.987013  \n",
       "10   1.000000  0.974359  0.987013  \n",
       "11   1.000000  0.974359  0.987013  \n",
       "12   0.000000  0.000000  0.000000  \n",
       "13   0.000000  0.000000  0.000000  \n",
       "14   0.000000  0.000000  0.000000  \n",
       "15   0.222222  0.666667  0.333333  \n",
       "16   0.000000  0.000000  0.000000  \n",
       "17   0.000000  0.000000  0.000000  \n",
       "18   0.000000  0.000000  0.000000  \n",
       "19   0.609504  0.480456  0.537341  \n",
       "20   0.406433  0.413593  0.391725  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rdflib import Graph, Namespace, RDF, Literal, XSD\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_fgf_metrics(ground_truth: Graph, generated: Graph):\n",
    "    # Step 1: Gather all triples per predicate\n",
    "    gt_by_pred = defaultdict(set)\n",
    "    gen_by_pred = defaultdict(set)\n",
    "\n",
    "    for s, p, o in ground_truth:\n",
    "        gt_by_pred[p].add((s, p, o))\n",
    "    for s, p, o in generated:\n",
    "        gen_by_pred[p].add((s, p, o))\n",
    "\n",
    "    # Step 2: Identify all predicates across both graphs\n",
    "    all_predicates = set(gt_by_pred.keys()) | set(gen_by_pred.keys())\n",
    "\n",
    "    # Step 3: Compute per-predicate TP, FP, FN and metrics\n",
    "    records = []\n",
    "    total_tp = total_fp = total_fn = 0\n",
    "\n",
    "    for pred in all_predicates:\n",
    "        gt_triples = gt_by_pred.get(pred, set())\n",
    "        gen_triples = gen_by_pred.get(pred, set())\n",
    "\n",
    "        tp = len(gt_triples & gen_triples)\n",
    "        fp = len(gen_triples - gt_triples)\n",
    "        fn = len(gt_triples - gen_triples)\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "        total_tp += tp\n",
    "        total_fp += fp\n",
    "        total_fn += fn\n",
    "\n",
    "        records.append({\n",
    "            \"predicate\": str(pred),\n",
    "            \"TP\": tp,\n",
    "            \"FP\": fp,\n",
    "            \"FN\": fn,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1\": f1\n",
    "        })\n",
    "\n",
    "    # Step 4: Compute micro-average\n",
    "    micro_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0\n",
    "    micro_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0\n",
    "    micro_f1 = 2 * micro_precision * micro_recall / (micro_precision + micro_recall) if (micro_precision + micro_recall) > 0 else 0.0\n",
    "\n",
    "    # Step 5: Compute macro-average\n",
    "    macro_precision = sum(r[\"Precision\"] for r in records) / len(records)\n",
    "    macro_recall = sum(r[\"Recall\"] for r in records) / len(records)\n",
    "    macro_f1 = sum(r[\"F1\"] for r in records) / len(records)\n",
    "\n",
    "    # Add summary row\n",
    "    records.append({\n",
    "        \"predicate\": \"MICRO_AVG\",\n",
    "        \"TP\": total_tp,\n",
    "        \"FP\": total_fp,\n",
    "        \"FN\": total_fn,\n",
    "        \"Precision\": micro_precision,\n",
    "        \"Recall\": micro_recall,\n",
    "        \"F1\": micro_f1\n",
    "    })\n",
    "    records.append({\n",
    "        \"predicate\": \"MACRO_AVG\",\n",
    "        \"TP\": None,\n",
    "        \"FP\": None,\n",
    "        \"FN\": None,\n",
    "        \"Precision\": macro_precision,\n",
    "        \"Recall\": macro_recall,\n",
    "        \"F1\": macro_f1\n",
    "    })\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "gt_graph = Graph()\n",
    "gt_graph.parse(\"ground_truth_target_graph.nt\", format=\"nt\")\n",
    "\n",
    "# Define your namespace mappings\n",
    "ground_truth_ns = \"https://loinc/measurement/\"\n",
    "generated_ns = \"http://example.org/tgt#\"\n",
    "\n",
    "# Prepare subject mapping: order-based matching\n",
    "src_subjects = sorted(set(src_graph.subjects()), key=lambda x: str(x))\n",
    "gt_subjects = sorted(set(gt_graph.subjects()), key=lambda x: str(x))\n",
    "subject_map = dict(zip(gt_subjects, src_subjects))\n",
    "\n",
    "# Rewrite all triple components: subject alignment + namespace normalization\n",
    "aligned_gt = Graph()\n",
    "for s, p, o in gt_graph:\n",
    "    # Subject replacement (by aligned subject if available)\n",
    "    s = subject_map.get(s, s)\n",
    "\n",
    "    # Namespace normalization\n",
    "    def replace_ns(term):\n",
    "        if isinstance(term, URIRef) and str(term).startswith(ground_truth_ns):\n",
    "            return URIRef(str(term).replace(ground_truth_ns, generated_ns))\n",
    "        return term\n",
    "\n",
    "    new_s = replace_ns(s)\n",
    "    new_p = replace_ns(p)\n",
    "    new_o = replace_ns(o)\n",
    "\n",
    "    aligned_gt.add((new_s, new_p, new_o))\n",
    "\n",
    "# Find all unique measurement subjects\n",
    "measurement_subjects = set(aligned_gt.subjects())\n",
    "\n",
    "# Create a new graph for type declarations\n",
    "type_graph = Graph()\n",
    "\n",
    "# Add rdf:type triples for each subject\n",
    "for subj in measurement_subjects:\n",
    "    type_graph.add((subj, RDF.type, TGT.Measurement))\n",
    "\n",
    "aligned_gt += type_graph\n",
    "\n",
    "# Remove all triples with the bad URI, and re-add them with the corrected URI\n",
    "def remove_trailing_slash_from_properties(graph):\n",
    "    to_add = []\n",
    "    to_remove = []\n",
    "\n",
    "    for s, p, o in graph:\n",
    "        if str(p).endswith(\"/\"):\n",
    "            new_p = URIRef(str(p).rstrip(\"/\"))\n",
    "            to_remove.append((s, p, o))\n",
    "            to_add.append((s, new_p, o))\n",
    "\n",
    "    for triple in to_remove:\n",
    "        graph.remove(triple)\n",
    "    for triple in to_add:\n",
    "        graph.add(triple)\n",
    "\n",
    "    return graph\n",
    "\n",
    "def replace_subject_namespace(graph, old_ns, new_ns):\n",
    "    updated_graph = graph.__class__()  # same type (likely rdflib.Graph)\n",
    "\n",
    "    for s, p, o in graph:\n",
    "        # Only update if subject starts with old namespace\n",
    "        if isinstance(s, URIRef) and str(s).startswith(str(old_ns)):\n",
    "            new_s = URIRef(str(s).replace(str(old_ns), str(new_ns), 1))\n",
    "        else:\n",
    "            new_s = s\n",
    "        updated_graph.add((new_s, p, o))\n",
    "\n",
    "    return updated_graph\n",
    "\n",
    "aligned_gt = remove_trailing_slash_from_properties(aligned_gt)\n",
    "aligned_gt = replace_subject_namespace(aligned_gt, SRC, TGT)\n",
    "\n",
    "tgt_graph = replace_subject_namespace(tgt_graph, SRC, TGT)\n",
    "\n",
    "df_results = evaluate_fgf_metrics(aligned_gt, tgt_graph)\n",
    "display(df_results)  # or df_results.to_csv(\"eval_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3cde970e-2ced-4b67-883c-30e00a76b638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "predicate & Precision & Recall & F1 \\\\\n",
      "\\midrule\n",
      "measurement\\_datetime & 1.00 & 0.35 & 0.51 \\\\\n",
      "provider\\_id & 1.00 & 0.97 & 0.99 \\\\\n",
      "measurement\\_source\\_concept\\_id & 0.00 & 0.00 & 0.00 \\\\\n",
      "measurement\\_source\\_value & 0.50 & 0.97 & 0.66 \\\\\n",
      "unit\\_concept\\_id & 0.00 & 0.00 & 0.00 \\\\\n",
      "measurement\\_id & 0.00 & 0.00 & 0.00 \\\\\n",
      "unit\\_source\\_value & 1.00 & 1.00 & 1.00 \\\\\n",
      "person\\_id & 1.00 & 0.97 & 0.99 \\\\\n",
      "measurement\\_concept\\_id & 0.00 & 0.00 & 0.00 \\\\\n",
      "measurement\\_time & 1.00 & 0.97 & 0.99 \\\\\n",
      "type & 1.00 & 0.97 & 0.99 \\\\\n",
      "measurement\\_date & 1.00 & 0.97 & 0.99 \\\\\n",
      "range\\_high & 0.00 & 0.00 & 0.00 \\\\\n",
      "range\\_low & 0.00 & 0.00 & 0.00 \\\\\n",
      "value\\_as\\_number & 0.00 & 0.00 & 0.00 \\\\\n",
      "value\\_as\\_concept\\_id & 0.22 & 0.67 & 0.33 \\\\\n",
      "measurement\\_type\\_concept\\_id & 0.00 & 0.00 & 0.00 \\\\\n",
      "value\\_source\\_value & 0.00 & 0.00 & 0.00 \\\\\n",
      "operator\\_concept\\_id & 0.00 & 0.00 & 0.00 \\\\\n",
      "MICRO\\_AVG & 0.61 & 0.48 & 0.54 \\\\\n",
      "MACRO\\_AVG & 0.41 & 0.41 & 0.39 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_metrics = (\n",
    "    df_results\n",
    "        # 1Ô∏è  keep only the local name after the last ‚Äú/‚Äù or ‚Äú#‚Äù\n",
    "        .assign(predicate=lambda d:\n",
    "                d[\"predicate\"].str.replace(r\".*[/#]\", \"\", regex=True))\n",
    "\n",
    "        # 2Ô∏è  drop the raw count columns\n",
    "        .drop(columns=[\"TP\", \"FP\", \"FN\"], errors=\"ignore\")\n",
    "\n",
    "        # 3Ô∏è  round/format *only* for presentation\n",
    "        .round(2)           # keeps the numeric dtype\n",
    ")\n",
    "\n",
    "latex_code = df_metrics.to_latex(index=False,  # drop the row numbers\n",
    "                         escape=True, # keep e.g. % or _ as-is\n",
    "                         float_format=lambda x: f\"{x:.2f}\",\n",
    "                         na_rep=\"\")    # how to show NaNs\n",
    "print(latex_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5ff324-582c-433a-b53f-bb5884aca3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
